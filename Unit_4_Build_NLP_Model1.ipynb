{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is a model for NLP that allows us to clean data and correctly categorize each post from reddit as it was classified.  Step by step the model will:\n",
    "1) clean and process data\n",
    "2) vectorize data\n",
    "3) fit a model with spacy\n",
    "4) score the model with a confusion matrix\n",
    "5) test the model\n",
    "6) pickle the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tcnick12\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data, mined by Jonathan\n",
    "url = 'https://raw.githubusercontent.com/BW-Post-Here-01/DS/master/Data/reddit_data_slimmed.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>COMMUNITY ANNOUNCEMENT In solidarity with the ...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Weekly r/Tattoos Question/FreeTalk Thread! - A...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Enter Shikari and Architects album artwork and...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>David Bowie Portrait - Healed, Done in April 2...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Photo realism artist chicago As the title sugg...</td>\n",
       "      <td>tattoos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content subreddit\n",
       "0  COMMUNITY ANNOUNCEMENT In solidarity with the ...   tattoos\n",
       "1  Weekly r/Tattoos Question/FreeTalk Thread! - A...   tattoos\n",
       "2  Enter Shikari and Architects album artwork and...   tattoos\n",
       "3  David Bowie Portrait - Healed, Done in April 2...   tattoos\n",
       "4  Photo realism artist chicago As the title sugg...   tattoos"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data with this function\n",
    "def cleaning_fn(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    4. Returns in lowercase.\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    clean = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    clean = ''.join(clean)\n",
    "    \n",
    "    clean = clean.lower()\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in clean.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the rows of the df so we don't have the iloc 1-100 all classified\n",
    "# as one class, the next 200 as another, etc., so we don't have issues with\n",
    "# a train/test split\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ELI5: When washing my hands, why does really h...</td>\n",
       "      <td>explainlikeimfive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I panicked when I lost my son at Disney World,...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>TIFU by permanently burning myself trying to c...</td>\n",
       "      <td>tifu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Last dance documentary for Korea So the last d...</td>\n",
       "      <td>nba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I Was Forced to be a Mother to a Doll Despite ...</td>\n",
       "      <td>nosleep</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content          subreddit\n",
       "0  ELI5: When washing my hands, why does really h...  explainlikeimfive\n",
       "1  I panicked when I lost my son at Disney World,...            nosleep\n",
       "2  TIFU by permanently burning myself trying to c...               tifu\n",
       "3  Last dance documentary for Korea So the last d...                nba\n",
       "4  I Was Forced to be a Mother to a Doll Despite ...            nosleep"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the df['subreddit'] is no longer grouped by class but it sorted at random.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The actual predictive model with three examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24048,)\n",
      "(8017,)\n",
      "(24048,)\n",
      "(8017,)\n"
     ]
    }
   ],
   "source": [
    "# Apply train/test split\n",
    "X = df['content']\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(analyzer=cleaning_fn)),\n",
    "    ('classifier', RandomForestClassifier()),  # Originally trained with MulinomialNB() but had low accuracy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tcnick12\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer=<function cleaning_fn at 0x000002AE0593D4C8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit X_train and y_train on the pipe\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tcnick12\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "          Android       0.31      0.28      0.29        76\n",
      "              DIY       0.07      0.04      0.05        50\n",
      "          Fitness       0.53      0.63      0.57       300\n",
      "       Futurology       0.16      0.08      0.11        37\n",
      "            Games       0.19      0.10      0.13        41\n",
      "     GetMotivated       0.40      0.24      0.30        17\n",
      "             IAmA       0.64      0.92      0.76       313\n",
      "            Jokes       0.24      0.46      0.31       462\n",
      "      LifeProTips       0.69      0.83      0.75       203\n",
      "  MachineLearning       0.66      0.70      0.68       198\n",
      "            Music       0.38      0.14      0.21        97\n",
      "        Overwatch       0.37      0.21      0.27       112\n",
      "              PS4       0.41      0.49      0.45       179\n",
      "   Showerthoughts       0.25      0.02      0.04        42\n",
      "           Tinder       0.50      0.16      0.24        25\n",
      "  TwoXChromosomes       0.28      0.23      0.25       330\n",
      "   WritingPrompts       0.79      0.65      0.71        40\n",
      "       askscience       0.33      0.28      0.30       316\n",
      "          atheism       0.73      0.52      0.61       166\n",
      "            books       0.68      0.44      0.53       204\n",
      "         buildapc       0.58      0.79      0.66       448\n",
      "         dadjokes       0.31      0.66      0.42       421\n",
      "           europe       1.00      0.12      0.22         8\n",
      "explainlikeimfive       0.85      0.74      0.79       163\n",
      "          gadgets       0.00      0.00      0.00         8\n",
      "    gameofthrones       0.75      0.47      0.58       104\n",
      "           gaming       0.19      0.06      0.10        62\n",
      "          history       0.65      0.53      0.58       352\n",
      "  leagueoflegends       0.56      0.40      0.46       237\n",
      "        lifehacks       0.00      0.00      0.00        40\n",
      "     listentothis       0.50      0.09      0.15        23\n",
      "malefashionadvice       0.88      0.48      0.62        87\n",
      "           movies       0.55      0.23      0.32        93\n",
      "              nba       0.74      0.59      0.65       186\n",
      "          nosleep       0.81      0.92      0.86       469\n",
      "     pcmasterrace       0.09      0.03      0.05       128\n",
      "  personalfinance       0.69      0.69      0.69       475\n",
      "       philosophy       1.00      0.07      0.12        15\n",
      "          pokemon       0.92      0.39      0.55        87\n",
      "         politics       1.00      0.74      0.85        27\n",
      "    relationships       0.60      0.65      0.62       359\n",
      "           soccer       1.00      0.27      0.42        30\n",
      "            space       0.50      0.05      0.09        41\n",
      "          tattoos       1.00      0.33      0.50         3\n",
      "       technology       1.00      0.11      0.20         9\n",
      "       television       0.54      0.16      0.24        90\n",
      "             tifu       0.69      0.64      0.66       404\n",
      "           travel       0.70      0.47      0.56       183\n",
      "            trees       0.27      0.09      0.13        68\n",
      "           webdev       0.74      0.30      0.43       189\n",
      "\n",
      "         accuracy                           0.53      8017\n",
      "        macro avg       0.55      0.37      0.40      8017\n",
      "     weighted avg       0.55      0.53      0.52      8017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline prediction object with X_test\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Score the model with X_test and y_test\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['explainlikeimfive', 'nosleep', 'tifu', 'nba', 'personalfinance',\n",
       "       'television', 'leagueoflegends', 'Jokes', 'Fitness', 'buildapc',\n",
       "       'TwoXChromosomes', 'relationships', 'gaming', 'pokemon',\n",
       "       'dadjokes', 'askscience', 'webdev', 'travel', 'atheism', 'history',\n",
       "       'lifehacks', 'Overwatch', 'LifeProTips', 'PS4', 'MachineLearning',\n",
       "       'IAmA', 'Music', 'gadgets', 'malefashionadvice', 'space', 'books',\n",
       "       'pcmasterrace', 'Futurology', 'DIY', 'Games', 'GetMotivated',\n",
       "       'trees', 'movies', 'gameofthrones', 'WritingPrompts', 'Android',\n",
       "       'soccer', 'listentothis', 'Tinder', 'philosophy', 'tattoos',\n",
       "       'Showerthoughts', 'technology', 'politics', 'europe'], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at these categories, try four fake reviews and see how the model does:\n",
    "df.subreddit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fn that takes in a reddit post and returns the top five most likely categories:\n",
    "def get_predictions(post, num_answers=5):\n",
    "  \"\"\" takes a post and returns the top categories it fits in \"\"\"\n",
    "\n",
    "  # get the predicted probabilities for each class\n",
    "  preds = pd.Series(pipeline.predict_proba(post)[0])\n",
    "\n",
    "  # save each class to the Series index\n",
    "  preds.index = pipeline.classes_\n",
    "\n",
    "  # sort to get the most likely classes\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top num_answers results in dict format\n",
    "  return preds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test one with a fake review about history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a fake review\n",
    "history_post = [ \"\"\"\n",
    "                History if my favorite subject.  I love to read historical accounts about ancient Rome and Greece.\n",
    "                I'm also a big World War 2 buff and I collect objects with historical significance.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "history              0.5\n",
       "PS4                  0.2\n",
       "Fitness              0.1\n",
       "nba                  0.1\n",
       "explainlikeimfive    0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(history_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test two with a fake review about pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again and mention pokemon to see if the model correctly guesses pokemon:\n",
    "pokemon_post = [ \"\"\"\n",
    "                My favorite pokemon are pikachu and charizard.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pokemon     0.4\n",
       "dadjokes    0.3\n",
       "Jokes       0.3\n",
       "webdev      0.0\n",
       "PS4         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(pokemon_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test three with a fake post about android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a fake prediction to see if android gets predicted:\n",
    "android_post = [ \"\"\"\n",
    "                I use a galaxy note 5.  My favorite opperating system version was oreo.\n",
    "                Android phones are better than iphones. I like to create apps for the app store.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lifehacks      0.3\n",
       "Android        0.2\n",
       "nba            0.1\n",
       "Jokes          0.1\n",
       "LifeProTips    0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(android_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test four with a fake post about music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a fake prediction to see if music gets predicted:\n",
    "music_post = [ \"\"\"\n",
    "                I love to listen to music.  My favorite singer/songwriter is Foy Vance.  Every so often\n",
    "                I like to listen to Bob Marley.  I have a large vinyl music collection but more recently I've\n",
    "                been listening to everything on Spotify.\n",
    "                \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Music      0.3\n",
       "webdev     0.1\n",
       "Games      0.1\n",
       "history    0.1\n",
       "nba        0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predictions(music_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "# save the model\n",
    "dump(pipeline, open('reddit_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to load in the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "# load the model\n",
    "loaded_model = load(open('reddit_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Flask app API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code meant to be in a Flask app.  Won't run on colab\n",
    "\n",
    "from pickle import load\n",
    "# load the model\n",
    "loaded_model = load(open('reddit_model.pkl', 'rb'))\n",
    "\n",
    "\n",
    "from flask import jsonify\n",
    "\n",
    "@app.route(\"/predict.json\", methods=[\"POST\"])\n",
    "def predict():\n",
    "  print(\"PREDICT ROUTE...\")\n",
    "  print(\"FORM DATA:\", dict(request.form))\n",
    "  #> {'title': 'example title', 'text': 'Example reddit post text here'}\n",
    "\n",
    "  # concatenate title and text, passed in as one variable to the model\n",
    "  post = request.form[\"title\"] + ' ' + screen_name_b = request.form[\"text\"]\n",
    "\n",
    "  # get predictions, store as a Pandas Series\n",
    "  preds = pd.Series(loaded_model.predict_proba(music_post)[0])\n",
    "\n",
    "  # assign the subreddit classes to the index\n",
    "  preds.index = loaded_model.classes_\n",
    "\n",
    "  # sort by values to get the top results\n",
    "  preds = preds.sort_values(ascending=False)\n",
    "\n",
    "  # return the top 5 results as JSON\n",
    "  return jsonify(subreddits=preds.index[:5],\n",
    "                  probabilities=preds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
